Maker Culture meets Art

There is a new conjunction emerging around open hardware, maker culture, and art. Remember how pop culture changed with the advent of punk in the late seventies?  We seem to be witnessing a similar explosion of 'garageband' creativity in the tennies, and the Raspberry Pi is proudly leading the charge for general purpose open source devices in this context.  (The Raspberry Pi Foundation even has an artist in residence — Rachel Rayns.)

Open-source hardware allows people to make their own robots, cameras or even electrocardiagraph machines by downloading schematics and incorporating any changes they need — and, typically, free open-source software is available to run these projects.  3D printers, laser cutters and CNC routers have helped this adoption of the open-everything ethos.

There are stacks of DIY projects based on the Raspberry Pi, and the flood shows no sign of slowing. The Raspberry Pi is the first general purpose device (in contrast to the magnificent, but more specialised Arduino), which is very easy to cobble together with add-on electronics. The thriving community that has grown up around the Raspberry Pi (including this magazine) is making a huge impact, from changes in the UK school curriculum to the Onion Pi anti-surveillance device and BrickPi's new brains for old lego Robots.

This article describes the PiBrush, a simple on-screen painting system that uses the XLoBorg motion and direction sensor add-on board from PiBorg. The XLoBorg adds an accelerometer and a magnetometer (compass) to your Pi and makes all sorts of motion-based interaction possible — like a Kinect, but free and open.  The header  graphic for this article was created with the PiBrush.

The PiBrush is an interactive art and technology exhibit (a rather grand name for something so small) that simulates flicking paint off the end of a paintbrush onto canvas — as Jackson Pollock famously did in the 1940s and 50s.  The setup includes two Raspberry Pis (one Model B and one Model A), one XLoBorg, a battery pack, a display and two Wi-Fi dongles. The Model A is held by the user and waved around. It collects acceleration data with the XLoBorg.  These data are then transmitted via Wi-Fi to the Model B, which processes the data collected into paint droplets and displays them on the screen. 

Functionally it looks like this:
and here is the hardware:  
We may improve on the elastic band in future models.  The Model A (client) top and the Model B (server) bottom, where the red button on the server saves the picture and starts a new picture.

The Code

The hardware for this project is fairly straightforward.  The XLoBorg plugs directly into the GPIO pins as a block on the client, and a push button simply connects a ground and GPIO pin on the server.  The software is where the challenge lies: intermediate to advanced code is needed, using Python and some basic physics.

The code is available on GitHub at:
https://github.com/hamishcunningham/pi-tronics/tree/master/pibrush/bin
The core is in accel_client.py which runs on the Model A and accel_server.py which runs on the Model B. The former reads from the sensor and sends data across the network; the latter processes the data and displays the output on the screen.

accel_server.py

The server script is the meat of the programming as it handles the physics simulation and display.  First, as we're dealing with sensor input we want to use something called a moving average to store accelerometer readings, which is initialised like this:

 # length of moving average array
 AL = 20

 # accelerometer storage for moving average
 AXa = numpy.zeros((AL, 1))
 AYa = numpy.zeros((AL, 1))
 AZa = numpy.ones((AL, 1))

 # array index for accelerometer data
 Ai = 0

A moving average is the average of, in this case, the last 20 readings. Some sort of filtering is almost always necessary when dealing with the sort of analog input data that comes from an accelerometer to prevent sudden jumps in the readings from having an undue impact on the processing. A moving average is a convenient and easy way to do it. We simply input the current reading at index Ai and then increment Ai until it exceeds AL, then wrap it back around:

 AXa[Ai] = float(a[0])
 AYa[Ai] = float(a[1])
 AZa[Ai] = float(a[2])
 Ai = Ai + 1
 if Ai == AL:
  Ai = 0

This is an array of the acclerometer data, as read over the network. I have used NumPy, because it executes more quickly than standard Python lists and there are convenient functions to make things simpler like .ones() to initialise all of AZa to a value of one — we assume a starting 1G of gravity downwards.

Functions

If you don't know, a function is a bit of code written in such a way that it can be executed from different places in the main program. Each call you can pass different arguments (input parameters) that will be operated on and a new result returned. There are two quite important functions used in the code that have to do with polar coordinates. If you know X, Y, Z, those are Cartesian coordinates.

 def polar(X, Y, Z):
   x = numpy.linalg.norm([X, Y, Z])
   if (x > 0):
     y = -math.atan2(Z, X)
     z = math.asin(Y / x)
   else:
     y = 0
     z = 0
   return (x, y, z)

Here polar takes a Cartesian X, Y, and Z coordinate and returns the equivalent polar coordinates, where x is R (the radius to the point from the origin), and y is A and z is B. The latter are the angular rotation about the Z and Y axis. Because the Model A may be rotated relative to the screen, we need a convenient mechanism for rotating the acceleration vectors recorded to ones we can use on the screen. In order to do that though we need to subtract the Earth's gravitational field. This is what the polar coordinates are used for. It's an extension of the regular subtraction of vectors. This function was based on this forum thread.

 def cartesian(X, A, B):
  x = 0 # don't bother - isn't used
  y = X * math.sin(B) * math.sin(A)
  z = X * math.cos(B)
  return (x, y, z)

Here cartesian, as you might suspect, does the opposite of polar, taking the distance X and rotations A and B and turns them back into Cartesian coordinates.  As this code is only used as part of getting coordinates ready for the screen, x, as the coordinate into the screen, is permanently set to 0.  This is an optimization to help the code run better on the Pi. The code here is based on this explanation of Cartesian and polar coordinates.

Main program

The main program consists of an infinitely repeating loop that reads the accelerometer data over the network, applies the earlier moving average, and then processes it.

 # move time forward
 dt = time.time() - last_time
 last_time = time.time()

The first thing to be done each loop is to move time forwards. Ideally execution time for each loop is identical and dt (the timestep) is constant. However, this isn't the case, so by knowing how long it's been since the last loop, we can check how far to move things, i.e., distance = velocity * time.

 # moving averages for acceleration
 AX = numpy.sum(AXa) / AL
 AY = numpy.sum(AYa) / AL
 AZ = numpy.sum(AZa) / AL

 # combined acceleration for
 # working out resting gravity
 A = math.fabs(numpy.linalg.norm([AX,
   AY, AZ]) - 1)

After reading in the accelerometer data and putting it in the AXa, etc., arrays, we then need to take the average of that array. .sum() adds up the values in an array.  Then to perform an average, we divide by the number of elements AL.  Therefore, AX, AY and AZ contain the moving average.

The total combined acceleration A is worked out by calculating the Euclidean distance from 0 to the acceleration vector position using linalg.norm(). At rest this should work out to just about 1 (remember we're working in acceleration in G(ravities), which is why we subtract 1. We then use .fabs() so that we always have a positive result which indicates the difference between acceleration due to gravity and the experienced acceleration. At rest this number should be very small.

 # in a slow moment store most recent
 # direction of the gravitational field
 if A < 0.02 and (last_time - last_G) > 0.12:
   GX = AX
   GY = AY
   GZ = AZ
   (PGR, PGA, PGB) = polar(GX, GY, GZ)
   last_G = last_time

 # rotate to screen coordinates
 # and subtract gravity
 (PAR, PAA, PAB) = polar(AX, AY, AZ)
 (GAX, GAY, GAZ) = cartesian(PAR,
   PAA - PGA + PSGA, PAB - PGB + PSGB)
 GAZ = GAZ - PGR

Now that we know something about gravity and the current acceleration, we can act on it. I've pointed out how in order to know which way the Model A is moving, we need to know where gravity is first.  We can't know it exactly, but we can estimate it.  Since A is the acceleration the Model A is experiencing excluding gravity, it's very low when the Model A isn't moving at all. This means that the only acceleration being experienced is due to gravity.  Therefore, we can take the estimate and turn it into polar coordinates.

For every loop we need to actually do the gravity subtraction and rotation for the screen.  We turn the current acceleration into polar coordinates.  Then on the next line we need to turn them back into cartesian coordinates, while subtracting the estimated rotation of the gravitational field. The GAZ line after this subtracts gravity from its absolute direction.

Paintbrush physics

Perhaps the most interesting bit of what is going on within every loop it the paint brush physics.  This is the code that controls what happens on the screen.  Everything up to this point has been to define GAY and GAZ, two variables indicating horizontal and vertical acceleration relative to the screen.  Now we can interpret this acceleration and make something happen.

 # acceleration detection for paint strokes
 A = numpy.linalg.norm([GAY, GAZ])

After this, A is the total acceleration of the Model A, with respect to the screen, ignoring gravity.  We can use this number to detect roughly if we are speeding up or slowing down.  What happens after that?

 if fast == 1:
   # accelerate the paint brush
   VX = VX - GAY * dt * 170
   VY = VY - GAZ * dt * 170
   BX = BX + VX * dt * 120
   BY = BY + VY * dt * 120

This bit of code is actually responsible for moving the brush, and only when we think it's moving.  To get position from acceleration, we have to integrate twice.  Imagine you are in a car travelling down the motorway at 100 kph. In one hour you will have travelled 100 km. Easy?  That is one simple integration, going from velocity to displacement. Acceleration is the rate of change in velocity, e.g. the speed at which the needle of the speedometer in the car climbs.  So now imagine accelerating at 1 kph per second, after 10 seconds you'll be going 10 kph faster.  After this acceleration, instead of 100 kph you are now going at 110 kph.  To get the distance, you have to to integrate twice.  (Fun fact: if you kept up that acceleration, after an hour you'd be going 3,700 kph and would have traveled 36,370 km.  Or almost around the Earth.)

We increment the brush velocity by the acceleration, factored by the timestep to keep the animation smooth.  I have also added a factor of 170 to scale up the acceleration, so it displays nicely on the screen.  (This means that one pixel approximately represents 170 metres.)  The next integration increments the brush position by adding on the current velocity, also multiplied by the timestep and scaled, this time by 120.  (These values just work, but are physically nonsense.)

 # add splotches.... high velocity big
 # splotches far apart, low small close
 if P > 0:
   V = numpy.linalg.norm([VX, VY])
   S = S + V
   d = A * random.randint(3, 5) * 25 + V

Now that the paintbrush is moving, here comes the last and most important bit of code: making paint droplets.  This bit of code is only run while the paintbrush is moving and only if there is paint on the brush.  Vaguely, one expects that the further the brush has been swung, more paint should come off.  The faster the brush is moving should also cause more paint to come off.  V is calculated as the total velocity and S is a running displacement.  d is calculated as a rough estimtate of paint droplet spacing.

Paint droplets fly off due to two factors:

1) Fluid mechanics. Roughly, I'm speaking of the affect of what happens when you move a glass of water too quickly.

2) Air resistance. The force you feel acting on your hand when you stick it out the car window on a drive on a nice summer's day. 

Both of these factors are rather complex subjects.  Therefore, I have lumped them together as the produce similar results — paint flying off the brush.  d is made up of A (our acceleration) times a random factor which is the fluid dynamics bit, and V is added for the air resistance bit. The random factor makes things a bit more interesting, perhaps taking into account things like globs of paint or hairs in the brush getting tangled.  25 is the scaling factor this time. (This applies to the acceleration term only as velocity was already scaled before.)

 if S > d:
   S = S - d
   P = P - pow(A*4, 2) * math.pi
   pygame.draw.circle(screen, (COLR, COLG,
     COLB), (int(BX), int(BY)), int(A*45))
   draw = 1

If we've travelled further than the expected paint droplet seperation according to our approximate simulation, we need to draw a paint droplet!  We calculate the amount of paint that was in the droplet arbitrarily, as the acceleration times four, squared, times π (pi).  (This is the area of a circle formula.)  This is subtracted from P, the paint on the paintbrush variable. The paint droplet actually drawn on the screen is done using cricles, with the Pygame function draw.circle().  The drawing on the screen takes place with  a random colour, at the paintbrush position (BX, BY).  A*45 is the paint droplet radius, where 45 is another scaling factor.